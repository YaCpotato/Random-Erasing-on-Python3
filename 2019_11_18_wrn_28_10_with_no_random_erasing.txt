[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 11273131592511035678
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 3951530748093945488
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 13594142791233259940
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 11686998894444344650
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 3779505394094619601
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 3137911475386597077
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651589325
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 2269830434227662815
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 9481827795086639054
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 16465418140148006942
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 16655481715992848942
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 160)  0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 160)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 160)  0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 160)  0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 320)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 320)  0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 320)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 320)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 640)    0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 640)    0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 640)    0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8, 8, 640)    0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           6410        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,507,242
Trainable params: 36,489,290
Non-trainable params: 17,952
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/100
 - 78s - loss: 14.4887 - acc: 0.0994 - val_loss: 14.4415 - val_acc: 0.1038
Epoch 2/100
 - 70s - loss: 14.5131 - acc: 0.0996 - val_loss: 14.4418 - val_acc: 0.1038
Epoch 3/100
 - 70s - loss: 8.2454 - acc: 0.1941 - val_loss: 3.1935 - val_acc: 0.1334
Epoch 4/100
 - 70s - loss: 1.8034 - acc: 0.4187 - val_loss: 2.4100 - val_acc: 0.2628
Epoch 5/100
 - 70s - loss: 1.4201 - acc: 0.5130 - val_loss: 1.5522 - val_acc: 0.4728
Epoch 6/100
 - 70s - loss: 1.1929 - acc: 0.5832 - val_loss: 1.2365 - val_acc: 0.5608
Epoch 7/100
 - 69s - loss: 1.0166 - acc: 0.6390 - val_loss: 1.4718 - val_acc: 0.5292
Epoch 8/100
 - 70s - loss: 0.9009 - acc: 0.6813 - val_loss: 1.3642 - val_acc: 0.5440
Epoch 9/100
 - 70s - loss: 0.8147 - acc: 0.7120 - val_loss: 1.3043 - val_acc: 0.5810
Epoch 10/100
 - 70s - loss: 0.7403 - acc: 0.7402 - val_loss: 1.4091 - val_acc: 0.5784
Epoch 11/100
 - 70s - loss: 0.6802 - acc: 0.7607 - val_loss: 0.9456 - val_acc: 0.6722
Epoch 12/100
 - 69s - loss: 0.6282 - acc: 0.7773 - val_loss: 0.8476 - val_acc: 0.7160
Epoch 13/100
 - 69s - loss: 0.5754 - acc: 0.8011 - val_loss: 1.1250 - val_acc: 0.6358
Epoch 14/100
 - 70s - loss: 0.5359 - acc: 0.8136 - val_loss: 0.8277 - val_acc: 0.7240
Epoch 15/100
 - 70s - loss: 0.5008 - acc: 0.8273 - val_loss: 0.8607 - val_acc: 0.7266
Epoch 16/100
 - 70s - loss: 0.4653 - acc: 0.8381 - val_loss: 0.7562 - val_acc: 0.7456
Epoch 17/100
 - 70s - loss: 0.4303 - acc: 0.8509 - val_loss: 0.8510 - val_acc: 0.7328
Epoch 18/100
 - 69s - loss: 0.4019 - acc: 0.8596 - val_loss: 0.7476 - val_acc: 0.7612
Epoch 19/100
 - 69s - loss: 0.3774 - acc: 0.8695 - val_loss: 0.8126 - val_acc: 0.7392
Epoch 20/100
 - 69s - loss: 0.3482 - acc: 0.8780 - val_loss: 0.7309 - val_acc: 0.7676
Epoch 21/100
 - 69s - loss: 0.3214 - acc: 0.8882 - val_loss: 0.7801 - val_acc: 0.7666
Epoch 22/100
 - 69s - loss: 0.2991 - acc: 0.8961 - val_loss: 0.6437 - val_acc: 0.7890
Epoch 23/100
 - 69s - loss: 0.2751 - acc: 0.9061 - val_loss: 1.1401 - val_acc: 0.7046
Epoch 24/100
 - 69s - loss: 0.2560 - acc: 0.9115 - val_loss: 0.7619 - val_acc: 0.7646
Epoch 25/100
 - 69s - loss: 0.2358 - acc: 0.9191 - val_loss: 0.8701 - val_acc: 0.7568
Epoch 26/100
 - 70s - loss: 0.2135 - acc: 0.9260 - val_loss: 0.6321 - val_acc: 0.8056
Epoch 27/100
 - 70s - loss: 0.1974 - acc: 0.9341 - val_loss: 0.7759 - val_acc: 0.7822
Epoch 28/100
 - 69s - loss: 0.1818 - acc: 0.9393 - val_loss: 1.0378 - val_acc: 0.7286
Epoch 29/100
 - 69s - loss: 0.1647 - acc: 0.9445 - val_loss: 1.2354 - val_acc: 0.7188
Epoch 30/100
 - 70s - loss: 0.1535 - acc: 0.9490 - val_loss: 0.7829 - val_acc: 0.7816
Epoch 31/100
 - 70s - loss: 0.1401 - acc: 0.9518 - val_loss: 0.7289 - val_acc: 0.7972
Epoch 32/100
 - 69s - loss: 0.1263 - acc: 0.9584 - val_loss: 0.8508 - val_acc: 0.7932
Epoch 33/100
 - 69s - loss: 0.1131 - acc: 0.9626 - val_loss: 0.7567 - val_acc: 0.7962
Epoch 34/100
 - 69s - loss: 0.1080 - acc: 0.9646 - val_loss: 0.8127 - val_acc: 0.7882
Epoch 35/100
 - 70s - loss: 0.0943 - acc: 0.9698 - val_loss: 0.8644 - val_acc: 0.7966
Epoch 36/100
 - 70s - loss: 0.0879 - acc: 0.9714 - val_loss: 0.8129 - val_acc: 0.7942
Epoch 37/100
 - 70s - loss: 0.0801 - acc: 0.9748 - val_loss: 0.7712 - val_acc: 0.8040
Epoch 38/100
 - 70s - loss: 0.0763 - acc: 0.9750 - val_loss: 0.8103 - val_acc: 0.8082
Epoch 39/100
 - 70s - loss: 0.0700 - acc: 0.9780 - val_loss: 0.8667 - val_acc: 0.7972
Epoch 40/100
 - 70s - loss: 0.0652 - acc: 0.9796 - val_loss: 0.7379 - val_acc: 0.8182
Epoch 41/100
 - 69s - loss: 0.0563 - acc: 0.9835 - val_loss: 0.6816 - val_acc: 0.8390
Epoch 42/100
 - 70s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.7334 - val_acc: 0.8212
Epoch 43/100
 - 70s - loss: 0.0514 - acc: 0.9842 - val_loss: 0.6892 - val_acc: 0.8354
Epoch 44/100
 - 70s - loss: 0.0464 - acc: 0.9863 - val_loss: 0.6769 - val_acc: 0.8386
Epoch 45/100
 - 70s - loss: 0.0440 - acc: 0.9864 - val_loss: 0.8148 - val_acc: 0.8088
Epoch 46/100
 - 70s - loss: 0.0429 - acc: 0.9873 - val_loss: 0.7904 - val_acc: 0.8158
Epoch 47/100
 - 69s - loss: 0.0393 - acc: 0.9883 - val_loss: 0.7498 - val_acc: 0.8206
Epoch 48/100
 - 69s - loss: 0.0354 - acc: 0.9900 - val_loss: 0.7349 - val_acc: 0.8370
Epoch 49/100
 - 69s - loss: 0.0327 - acc: 0.9912 - val_loss: 0.9009 - val_acc: 0.8018
Epoch 50/100
 - 69s - loss: 0.0327 - acc: 0.9912 - val_loss: 0.8063 - val_acc: 0.8254
Epoch 51/100
 - 69s - loss: 0.0312 - acc: 0.9913 - val_loss: 0.8179 - val_acc: 0.8218
Epoch 52/100
 - 69s - loss: 0.0307 - acc: 0.9912 - val_loss: 0.7670 - val_acc: 0.8278
Epoch 53/100
 - 69s - loss: 0.0280 - acc: 0.9921 - val_loss: 0.7249 - val_acc: 0.8372
Epoch 54/100
 - 69s - loss: 0.0277 - acc: 0.9921 - val_loss: 0.8077 - val_acc: 0.8256
Epoch 55/100
 - 69s - loss: 0.0258 - acc: 0.9928 - val_loss: 0.7275 - val_acc: 0.8344
Epoch 56/100
 - 69s - loss: 0.0235 - acc: 0.9942 - val_loss: 0.7531 - val_acc: 0.8398
Epoch 57/100
 - 70s - loss: 0.0234 - acc: 0.9935 - val_loss: 0.8838 - val_acc: 0.8112
Epoch 58/100
 - 69s - loss: 0.0222 - acc: 0.9946 - val_loss: 0.8168 - val_acc: 0.8300
Epoch 59/100
 - 69s - loss: 0.0220 - acc: 0.9940 - val_loss: 0.7505 - val_acc: 0.8350
Epoch 60/100
 - 70s - loss: 0.0190 - acc: 0.9950 - val_loss: 0.8841 - val_acc: 0.8220
Epoch 61/100
 - 69s - loss: 0.0182 - acc: 0.9955 - val_loss: 0.8538 - val_acc: 0.8258
Epoch 62/100
 - 69s - loss: 0.0186 - acc: 0.9951 - val_loss: 0.7662 - val_acc: 0.8318
Epoch 63/100
 - 70s - loss: 0.0172 - acc: 0.9954 - val_loss: 0.8030 - val_acc: 0.8274
Epoch 64/100
 - 69s - loss: 0.0164 - acc: 0.9959 - val_loss: 0.7998 - val_acc: 0.8340
Epoch 65/100
 - 69s - loss: 0.0149 - acc: 0.9965 - val_loss: 0.7743 - val_acc: 0.8306
Epoch 66/100
 - 69s - loss: 0.0157 - acc: 0.9961 - val_loss: 0.7497 - val_acc: 0.8370
Epoch 67/100
 - 69s - loss: 0.0160 - acc: 0.9958 - val_loss: 0.9174 - val_acc: 0.8208
Epoch 68/100
 - 69s - loss: 0.0148 - acc: 0.9962 - val_loss: 0.7706 - val_acc: 0.8406
Epoch 69/100
 - 69s - loss: 0.0139 - acc: 0.9969 - val_loss: 0.8004 - val_acc: 0.8342
Epoch 70/100
 - 69s - loss: 0.0137 - acc: 0.9966 - val_loss: 0.8472 - val_acc: 0.8282
Epoch 71/100
 - 69s - loss: 0.0143 - acc: 0.9964 - val_loss: 0.8227 - val_acc: 0.8298
Epoch 72/100
 - 69s - loss: 0.0136 - acc: 0.9967 - val_loss: 0.8229 - val_acc: 0.8320
Epoch 73/100
 - 69s - loss: 0.0138 - acc: 0.9963 - val_loss: 0.8393 - val_acc: 0.8302
Epoch 74/100
 - 70s - loss: 0.0131 - acc: 0.9968 - val_loss: 0.7375 - val_acc: 0.8444
Epoch 75/100
 - 69s - loss: 0.0133 - acc: 0.9965 - val_loss: 0.7705 - val_acc: 0.8376
Epoch 76/100
 - 70s - loss: 0.0118 - acc: 0.9971 - val_loss: 0.8611 - val_acc: 0.8308
Epoch 77/100
 - 70s - loss: 0.0115 - acc: 0.9975 - val_loss: 0.7610 - val_acc: 0.8454
Epoch 78/100
 - 70s - loss: 0.0117 - acc: 0.9972 - val_loss: 0.8192 - val_acc: 0.8358
Epoch 79/100
 - 70s - loss: 0.0110 - acc: 0.9977 - val_loss: 0.8094 - val_acc: 0.8378
Epoch 80/100
 - 70s - loss: 0.0116 - acc: 0.9967 - val_loss: 0.7756 - val_acc: 0.8410
Epoch 81/100
 - 70s - loss: 0.0106 - acc: 0.9976 - val_loss: 0.7727 - val_acc: 0.8450
Epoch 82/100
 - 70s - loss: 0.0106 - acc: 0.9977 - val_loss: 0.8024 - val_acc: 0.8412
Epoch 83/100
 - 70s - loss: 0.0105 - acc: 0.9974 - val_loss: 0.7899 - val_acc: 0.8402
Epoch 84/100
 - 70s - loss: 0.0111 - acc: 0.9972 - val_loss: 0.8188 - val_acc: 0.8348
Epoch 85/100
 - 70s - loss: 0.0095 - acc: 0.9977 - val_loss: 0.7826 - val_acc: 0.8382
Epoch 86/100
 - 70s - loss: 0.0095 - acc: 0.9979 - val_loss: 0.7757 - val_acc: 0.8414
Epoch 87/100
 - 70s - loss: 0.0085 - acc: 0.9983 - val_loss: 0.7708 - val_acc: 0.8446
Epoch 88/100
 - 70s - loss: 0.0088 - acc: 0.9981 - val_loss: 0.7897 - val_acc: 0.8390
Epoch 89/100
 - 70s - loss: 0.0081 - acc: 0.9984 - val_loss: 0.7544 - val_acc: 0.8474
Epoch 90/100
 - 70s - loss: 0.0089 - acc: 0.9979 - val_loss: 0.8622 - val_acc: 0.8320
Epoch 91/100
 - 70s - loss: 0.0092 - acc: 0.9977 - val_loss: 0.7936 - val_acc: 0.8392
Epoch 92/100
 - 70s - loss: 0.0079 - acc: 0.9983 - val_loss: 0.8438 - val_acc: 0.8352
Epoch 93/100
 - 70s - loss: 0.0082 - acc: 0.9983 - val_loss: 0.7688 - val_acc: 0.8444
Epoch 94/100
 - 70s - loss: 0.0077 - acc: 0.9985 - val_loss: 0.7871 - val_acc: 0.8448
Epoch 95/100
 - 70s - loss: 0.0085 - acc: 0.9980 - val_loss: 0.7968 - val_acc: 0.8418
Epoch 96/100
 - 70s - loss: 0.0081 - acc: 0.9982 - val_loss: 0.7967 - val_acc: 0.8452
Epoch 97/100
 - 70s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.7816 - val_acc: 0.8444
Epoch 98/100
 - 70s - loss: 0.0070 - acc: 0.9986 - val_loss: 0.8441 - val_acc: 0.8322
Epoch 99/100
 - 70s - loss: 0.0074 - acc: 0.9985 - val_loss: 0.8454 - val_acc: 0.8368
Epoch 100/100
 - 70s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.8950 - val_acc: 0.8318
6963.71432518959
--------
{'val_loss': [14.441523083496094, 14.441767248535156, 3.193451824951172, 2.409976076889038, 1.552162265586853, 1.2365123691558837, 1.4718386436462403, 1.3641566835403443, 1.3042914752960204, 1.4090894680023194, 0.9455746011734009, 0.8476024150848389, 1.1250491847991944, 0.8276771890640259, 0.860685629272461, 0.7562092502593994, 0.8510087627410888, 0.7476015681266784, 0.8126301239013672, 0.7309361936569214, 0.7800817691802978, 0.6437178936004638, 1.140074530029297, 0.761899736404419, 0.8700893089294434, 0.63213207321167, 0.7758552835464477, 1.0377957996368408, 1.2354141868591308, 0.782870934677124, 0.7288572010993958, 0.8507535514831543, 0.75665283203125, 0.8126800842285157, 0.8643669095039368, 0.8128609841346741, 0.7712108112335205, 0.8102997873306275, 0.8666590662002563, 0.7378605498313904, 0.6815872907638549, 0.7333671080589295, 0.6892291227340698, 0.676891291809082, 0.8147620030403138, 0.7904331377029419, 0.7497781585693359, 0.7349397304534913, 0.9009264568328857, 0.8062548536300659, 0.8179418373107911, 0.7670269897460937, 0.7249474828720093, 0.8076534910202027, 0.727470690536499, 0.7530532821655274, 0.8838463451385498, 0.8168248271942139, 0.7505153074264527, 0.8840868324279785, 0.8537581428527832, 0.7661631664276123, 0.8030017146110535, 0.7998116397857666, 0.7742544460296631, 0.7496669382095337, 0.9174233526229858, 0.7706068952560425, 0.8003699401855469, 0.8472499401092529, 0.8227286832809448, 0.8228618654251099, 0.8393221816062927, 0.737500315284729, 0.7705334321975708, 0.861138478088379, 0.7610351703643798, 0.8192247085571289, 0.8094161403656006, 0.7755801677703857, 0.7727287498474121, 0.802429485321045, 0.7899238286972046, 0.8188283092498779, 0.7825583822250366, 0.7756634120941162, 0.7708453870773315, 0.7896970834732056, 0.7544211917877197, 0.8622140583992004, 0.7935707530975342, 0.8437592834472656, 0.7687796753883361, 0.7870649168014526, 0.7968003246307374, 0.7967304136276245, 0.7816410977363586, 0.8441100133895874, 0.845366422176361, 0.8950338466644288], 'val_acc': [0.1038, 0.1038, 0.1334, 0.2628, 0.4728, 0.5608, 0.5292, 0.544, 0.581, 0.5784, 0.6722, 0.716, 0.6358, 0.724, 0.7266, 0.7456, 0.7328, 0.7612, 0.7392, 0.7676, 0.7666, 0.789, 0.7046, 0.7646, 0.7568, 0.8056, 0.7822, 0.7286, 0.7188, 0.7816, 0.7972, 0.7932, 0.7962, 0.7882, 0.7966, 0.7942, 0.804, 0.8082, 0.7972, 0.8182, 0.839, 0.8212, 0.8354, 0.8386, 0.8088, 0.8158, 0.8206, 0.837, 0.8018, 0.8254, 0.8218, 0.8278, 0.8372, 0.8256, 0.8344, 0.8398, 0.8112, 0.83, 0.835, 0.822, 0.8258, 0.8318, 0.8274, 0.834, 0.8306, 0.837, 0.8208, 0.8406, 0.8342, 0.8282, 0.8298, 0.832, 0.8302, 0.8444, 0.8376, 0.8308, 0.8454, 0.8358, 0.8378, 0.841, 0.845, 0.8412, 0.8402, 0.8348, 0.8382, 0.8414, 0.8446, 0.839, 0.8474, 0.832, 0.8392, 0.8352, 0.8444, 0.8448, 0.8418, 0.8452, 0.8444, 0.8322, 0.8368, 0.8318], 'loss': [14.48871930609809, 14.513091127522786, 8.24535310283237, 1.8034266922208997, 1.4200631271150377, 1.1928618198182848, 1.016617611058553, 0.9008897968610128, 0.8146751893997193, 0.7403249373859829, 0.6801816227912902, 0.6281594008710649, 0.5753762136141459, 0.5358567941506703, 0.5008027453210618, 0.4652805501090156, 0.43030281817118327, 0.401858261373308, 0.37740403962665137, 0.3482454334841834, 0.32142039857440524, 0.2990765576044718, 0.2751432885858748, 0.2560050489425659, 0.23577334412998624, 0.21349014595879448, 0.19736795632309384, 0.18179922412898805, 0.16471345664660136, 0.15354963689380222, 0.140149081325531, 0.12626892931196423, 0.11313457392586602, 0.10797111172013812, 0.09425624195072387, 0.08785930936071608, 0.08006654234462314, 0.07630068458186255, 0.06995456435481708, 0.06515393473572201, 0.056345542467302744, 0.05448000612391366, 0.051408545987473596, 0.0463805313459701, 0.0439945892204841, 0.042892275763882534, 0.03928664166066382, 0.03543577058017254, 0.03267659500837326, 0.032714997781647576, 0.03122483807272381, 0.030650830523173016, 0.027973590879307852, 0.027662114408943388, 0.0258192025307152, 0.02346023551358117, 0.023410033321380617, 0.02217819063928392, 0.022006508609983655, 0.01896960659623146, 0.018244627030359374, 0.01864883926709493, 0.017207527301046584, 0.016372599497603047, 0.014886909736858474, 0.01566381099704239, 0.016025588904486764, 0.014836754807995425, 0.013930793528424369, 0.013665578644639916, 0.014266620075040393, 0.01357951228916645, 0.013772451959715949, 0.01314983739654223, 0.013317651823163032, 0.011795511102014117, 0.011460185930960709, 0.011674432184298833, 0.010975305985742145, 0.01157811116874218, 0.010605542846769095, 0.010592703777220514, 0.01045097785393397, 0.011090239373015032, 0.009473148162373239, 0.009494058693697054, 0.008457309783001741, 0.008760623020182053, 0.008076992101967335, 0.008850901369916068, 0.009168543842496971, 0.007850879881117078, 0.008209955355359448, 0.007732765673266517, 0.0084544665315085, 0.0081346542402688, 0.007913608192900816, 0.007049086060126623, 0.0074039096355438235, 0.00792211596634653], 'acc': [0.09937777778837416, 0.09957777778042687, 0.19408888886769612, 0.4186888888835907, 0.5130444444338481, 0.5832222222646077, 0.6389777778201633, 0.6813111111323039, 0.711977777756585, 0.7402444444020589, 0.7606888888465033, 0.7772666666984558, 0.8011333333121405, 0.8136444444232517, 0.8273333333651225, 0.838133333322737, 0.8508666667090522, 0.8595777777565851, 0.8695111110899183, 0.8780222222540114, 0.8881555555979411, 0.8961333333333333, 0.9061333333757189, 0.9114888888465034, 0.9190666666242812, 0.9260222221904331, 0.9340888889312744, 0.9392888888570997, 0.9444888889206781, 0.9490444444126553, 0.9517777777459886, 0.9583777778201633, 0.962555555597941, 0.9646444444232517, 0.9698444444126553, 0.9714222221798366, 0.9747999999576145, 0.9750444444126553, 0.9780222222116258, 0.9795777777777778, 0.9834888888782926, 0.9832666666560703, 0.9841777777777778, 0.9863111111111111, 0.9863777777671814, 0.9873111111005147, 0.9882888888782925, 0.9900444444444444, 0.9911555555449592, 0.9911777777671814, 0.9913111110899183, 0.9911999999894037, 0.9920666666560702, 0.9921111111005148, 0.9928444444444444, 0.9941777777777778, 0.993533333322737, 0.9945777777671814, 0.9940444444444444, 0.9950222222116258, 0.9955333333333334, 0.9950666666666667, 0.9954, 0.9959333333333333, 0.9964888888888889, 0.9960666666666667, 0.9957999999894036, 0.9962, 0.9968666666560703, 0.9966, 0.9963555555343628, 0.9967111110899184, 0.996333333322737, 0.9968, 0.9964666666560703, 0.9970666666454739, 0.9974666666666666, 0.9971555555449592, 0.9977111111111111, 0.9967333333333334, 0.9975555555555555, 0.9976666666666667, 0.9973999999894037, 0.9971555555555556, 0.9977333333333334, 0.9979111111111111, 0.9982888888888889, 0.9980888888888889, 0.9983777777777778, 0.9979111111005148, 0.9976666666666667, 0.9983111111111111, 0.9982666666666666, 0.9984666666666666, 0.9979777777777777, 0.9981777777777778, 0.9982444444444445, 0.9986444444126553, 0.9984666666560703, 0.9981555555449592]}
===Final Test Score===
Test loss: 0.9375871236875654
Test accuracy: 0.8297
