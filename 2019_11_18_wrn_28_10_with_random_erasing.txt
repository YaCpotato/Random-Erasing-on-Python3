[name: "/device:CPU:0"
device_type: "CPU"
memory_limit: 268435456
locality {
}
incarnation: 9155830212922061311
, name: "/device:XLA_GPU:0"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 12031017358190060812
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:1"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 13068599173323701648
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:2"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 13431074319802629958
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_GPU:3"
device_type: "XLA_GPU"
memory_limit: 17179869184
locality {
}
incarnation: 12204100723171741301
physical_device_desc: "device: XLA_GPU device"
, name: "/device:XLA_CPU:0"
device_type: "XLA_CPU"
memory_limit: 17179869184
locality {
}
incarnation: 8540498392151069835
physical_device_desc: "device: XLA_CPU device"
, name: "/device:GPU:0"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 5690077021131174991
physical_device_desc: "device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3d:00.0, compute capability: 7.0"
, name: "/device:GPU:1"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 4352864830452820002
physical_device_desc: "device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:3e:00.0, compute capability: 7.0"
, name: "/device:GPU:2"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 3
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 5213931304363423427
physical_device_desc: "device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b1:00.0, compute capability: 7.0"
, name: "/device:GPU:3"
device_type: "GPU"
memory_limit: 15651651584
locality {
  bus_id: 2
  numa_node: 1
  links {
    link {
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 1
      type: "StreamExecutor"
      strength: 1
    }
    link {
      device_id: 2
      type: "StreamExecutor"
      strength: 1
    }
  }
}
incarnation: 744896408983683247
physical_device_desc: "device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:b2:00.0, compute capability: 7.0"
]
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 160)  23200       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 160)  640         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 160)  0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 32, 160)  0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 160)  230560      dropout_1[0][0]                  
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 160)  2720        activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 160)  0           conv2d_2[0][0]                   
                                                                 activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 160)  230560      add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 160)  640         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 160)  0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 32, 32, 160)  0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 160)  230560      dropout_2[0][0]                  
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 160)  640         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 160)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 160)  0           add_1[0][0]                      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 160)  230560      add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 32, 32, 160)  0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 160)  230560      dropout_3[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 160)  640         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 160)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 160)  0           add_2[0][0]                      
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 160)  230560      add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 160)  640         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 160)  0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 32, 32, 160)  0           activation_8[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 160)  230560      dropout_4[0][0]                  
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 160)  640         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 160)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 160)  0           add_3[0][0]                      
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 160)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 320)  461120      max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 320)  1280        conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 320)  0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 16, 16, 320)  0           activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 320)  921920      dropout_5[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 320)  1280        conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 320)  51520       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 320)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 320)  0           conv2d_11[0][0]                  
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 320)  921920      add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 320)  1280        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 16, 16, 320)  0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 16, 16, 320)  0           activation_12[0][0]              
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 16, 16, 320)  921920      dropout_6[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 16, 16, 320)  1280        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 16, 16, 320)  0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
add_6 (Add)                     (None, 16, 16, 320)  0           add_5[0][0]                      
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 16, 16, 320)  921920      add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 16, 16, 320)  1280        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 16, 16, 320)  0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 16, 16, 320)  0           activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 16, 16, 320)  921920      dropout_7[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 16, 16, 320)  1280        conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 16, 16, 320)  0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
add_7 (Add)                     (None, 16, 16, 320)  0           add_6[0][0]                      
                                                                 activation_15[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 16, 16, 320)  921920      add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 16, 16, 320)  1280        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 16, 16, 320)  0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 16, 16, 320)  0           activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 16, 16, 320)  921920      dropout_8[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 16, 16, 320)  1280        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 16, 16, 320)  0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
add_8 (Add)                     (None, 16, 16, 320)  0           add_7[0][0]                      
                                                                 activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 320)    0           add_8[0][0]                      
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 8, 8, 640)    1843840     max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 8, 8, 640)    2560        conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 8, 8, 640)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 8, 8, 640)    0           activation_18[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_9[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 8, 8, 640)    2560        conv2d_22[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 8, 8, 640)    205440      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 8, 8, 640)    0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
add_9 (Add)                     (None, 8, 8, 640)    0           conv2d_20[0][0]                  
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 8, 8, 640)    3687040     add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 8, 8, 640)    2560        conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 8, 8, 640)    0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 8, 8, 640)    0           activation_20[0][0]              
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_10[0][0]                 
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 8, 8, 640)    2560        conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 8, 8, 640)    0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
add_10 (Add)                    (None, 8, 8, 640)    0           add_9[0][0]                      
                                                                 activation_21[0][0]              
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 8, 8, 640)    3687040     add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 8, 8, 640)    2560        conv2d_25[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 8, 8, 640)    0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 8, 8, 640)    0           activation_22[0][0]              
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_11[0][0]                 
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 8, 8, 640)    2560        conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 8, 8, 640)    0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
add_11 (Add)                    (None, 8, 8, 640)    0           add_10[0][0]                     
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 8, 8, 640)    3687040     add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 8, 8, 640)    2560        conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 8, 8, 640)    0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 8, 8, 640)    0           activation_24[0][0]              
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 8, 8, 640)    3687040     dropout_12[0][0]                 
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 8, 8, 640)    2560        conv2d_28[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 8, 8, 640)    0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
add_12 (Add)                    (None, 8, 8, 640)    0           add_11[0][0]                     
                                                                 activation_25[0][0]              
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 640)          0           add_12[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           6410        global_average_pooling2d_1[0][0] 
==================================================================================================
Total params: 36,507,242
Trainable params: 36,489,290
Non-trainable params: 17,952
__________________________________________________________________________________________________
Train on 45000 samples, validate on 5000 samples
Epoch 1/100
 - 78s - loss: 3.5310 - acc: 0.3079 - val_loss: 1.6201 - val_acc: 0.4150
Epoch 2/100
 - 70s - loss: 1.5455 - acc: 0.4749 - val_loss: 1.5720 - val_acc: 0.4340
Epoch 3/100
 - 70s - loss: 1.3235 - acc: 0.5461 - val_loss: 1.8952 - val_acc: 0.4034
Epoch 4/100
 - 70s - loss: 1.1419 - acc: 0.5971 - val_loss: 1.2465 - val_acc: 0.5830
Epoch 5/100
 - 70s - loss: 0.9910 - acc: 0.6460 - val_loss: 1.6434 - val_acc: 0.4778
Epoch 6/100
 - 70s - loss: 0.8898 - acc: 0.6846 - val_loss: 1.0184 - val_acc: 0.6518
Epoch 7/100
 - 70s - loss: 0.8150 - acc: 0.7112 - val_loss: 0.9305 - val_acc: 0.6768
Epoch 8/100
 - 70s - loss: 0.7532 - acc: 0.7345 - val_loss: 0.8143 - val_acc: 0.7170
Epoch 9/100
 - 70s - loss: 0.6918 - acc: 0.7549 - val_loss: 1.2652 - val_acc: 0.5860
Epoch 10/100
 - 70s - loss: 0.6462 - acc: 0.7718 - val_loss: 1.0047 - val_acc: 0.6564
Epoch 11/100
 - 70s - loss: 0.5960 - acc: 0.7898 - val_loss: 0.8419 - val_acc: 0.7238
Epoch 12/100
 - 70s - loss: 0.5558 - acc: 0.8052 - val_loss: 1.0888 - val_acc: 0.6566
Epoch 13/100
 - 70s - loss: 0.5201 - acc: 0.8187 - val_loss: 0.8431 - val_acc: 0.7240
Epoch 14/100
 - 70s - loss: 0.4833 - acc: 0.8299 - val_loss: 1.0365 - val_acc: 0.6578
Epoch 15/100
 - 70s - loss: 0.4515 - acc: 0.8418 - val_loss: 0.9592 - val_acc: 0.7076
Epoch 16/100
 - 70s - loss: 0.4181 - acc: 0.8544 - val_loss: 0.6791 - val_acc: 0.7744
Epoch 17/100
 - 70s - loss: 0.3908 - acc: 0.8639 - val_loss: 0.9202 - val_acc: 0.7270
Epoch 18/100
 - 70s - loss: 0.3626 - acc: 0.8739 - val_loss: 0.7831 - val_acc: 0.7446
Epoch 19/100
 - 70s - loss: 0.3337 - acc: 0.8825 - val_loss: 0.8668 - val_acc: 0.7408
Epoch 20/100
 - 70s - loss: 0.3110 - acc: 0.8921 - val_loss: 0.7094 - val_acc: 0.7836
Epoch 21/100
 - 70s - loss: 0.2869 - acc: 0.9002 - val_loss: 0.7331 - val_acc: 0.7772
Epoch 22/100
 - 70s - loss: 0.2639 - acc: 0.9094 - val_loss: 0.7320 - val_acc: 0.7740
Epoch 23/100
 - 70s - loss: 0.2405 - acc: 0.9165 - val_loss: 0.9462 - val_acc: 0.7422
Epoch 24/100
 - 70s - loss: 0.2209 - acc: 0.9230 - val_loss: 1.0807 - val_acc: 0.7302
Epoch 25/100
 - 70s - loss: 0.2024 - acc: 0.9316 - val_loss: 0.7126 - val_acc: 0.7830
Epoch 26/100
 - 70s - loss: 0.1863 - acc: 0.9373 - val_loss: 0.7676 - val_acc: 0.7812
Epoch 27/100
 - 70s - loss: 0.1664 - acc: 0.9437 - val_loss: 0.9116 - val_acc: 0.7544
Epoch 28/100
 - 70s - loss: 0.1493 - acc: 0.9499 - val_loss: 0.7514 - val_acc: 0.7940
Epoch 29/100
 - 70s - loss: 0.1392 - acc: 0.9541 - val_loss: 0.7481 - val_acc: 0.7954
Epoch 30/100
 - 70s - loss: 0.1287 - acc: 0.9571 - val_loss: 0.8189 - val_acc: 0.7780
Epoch 31/100
 - 70s - loss: 0.1141 - acc: 0.9626 - val_loss: 1.6600 - val_acc: 0.6734
Epoch 32/100
 - 70s - loss: 0.1035 - acc: 0.9658 - val_loss: 0.9681 - val_acc: 0.7706
Epoch 33/100
 - 70s - loss: 0.0981 - acc: 0.9680 - val_loss: 0.8538 - val_acc: 0.7914
Epoch 34/100
 - 70s - loss: 0.0878 - acc: 0.9711 - val_loss: 0.7599 - val_acc: 0.8118
Epoch 35/100
 - 70s - loss: 0.0795 - acc: 0.9749 - val_loss: 0.7435 - val_acc: 0.8100
Epoch 36/100
 - 70s - loss: 0.0764 - acc: 0.9752 - val_loss: 0.8320 - val_acc: 0.8068
Epoch 37/100
 - 70s - loss: 0.0666 - acc: 0.9784 - val_loss: 0.9752 - val_acc: 0.7832
Epoch 38/100
 - 70s - loss: 0.0614 - acc: 0.9813 - val_loss: 0.7277 - val_acc: 0.8158
Epoch 39/100
 - 70s - loss: 0.0587 - acc: 0.9827 - val_loss: 0.8009 - val_acc: 0.8112
Epoch 40/100
 - 70s - loss: 0.0515 - acc: 0.9852 - val_loss: 0.7916 - val_acc: 0.8158
Epoch 41/100
 - 70s - loss: 0.0490 - acc: 0.9858 - val_loss: 0.8771 - val_acc: 0.8072
Epoch 42/100
 - 70s - loss: 0.0474 - acc: 0.9854 - val_loss: 0.7660 - val_acc: 0.8266
Epoch 43/100
 - 70s - loss: 0.0429 - acc: 0.9874 - val_loss: 1.0203 - val_acc: 0.7776
Epoch 44/100
 - 70s - loss: 0.0407 - acc: 0.9884 - val_loss: 1.1547 - val_acc: 0.7696
Epoch 45/100
 - 70s - loss: 0.0369 - acc: 0.9894 - val_loss: 1.0171 - val_acc: 0.7788
Epoch 46/100
 - 70s - loss: 0.0353 - acc: 0.9899 - val_loss: 0.7630 - val_acc: 0.8232
Epoch 47/100
 - 70s - loss: 0.0354 - acc: 0.9891 - val_loss: 0.8480 - val_acc: 0.8152
Epoch 48/100
 - 70s - loss: 0.0334 - acc: 0.9904 - val_loss: 0.9309 - val_acc: 0.7950
Epoch 49/100
 - 70s - loss: 0.0299 - acc: 0.9919 - val_loss: 0.7897 - val_acc: 0.8260
Epoch 50/100
 - 70s - loss: 0.0284 - acc: 0.9921 - val_loss: 0.8317 - val_acc: 0.8190
Epoch 51/100
 - 70s - loss: 0.0276 - acc: 0.9923 - val_loss: 0.8173 - val_acc: 0.8250
Epoch 52/100
 - 70s - loss: 0.0256 - acc: 0.9931 - val_loss: 0.8425 - val_acc: 0.8216
Epoch 53/100
 - 70s - loss: 0.0239 - acc: 0.9939 - val_loss: 0.8761 - val_acc: 0.8148
Epoch 54/100
 - 70s - loss: 0.0250 - acc: 0.9931 - val_loss: 0.7942 - val_acc: 0.8272
Epoch 55/100
 - 70s - loss: 0.0218 - acc: 0.9945 - val_loss: 0.8838 - val_acc: 0.8138
Epoch 56/100
 - 70s - loss: 0.0225 - acc: 0.9938 - val_loss: 0.7618 - val_acc: 0.8338
Epoch 57/100
 - 70s - loss: 0.0211 - acc: 0.9948 - val_loss: 1.0225 - val_acc: 0.7988
Epoch 58/100
 - 70s - loss: 0.0209 - acc: 0.9946 - val_loss: 0.7578 - val_acc: 0.8372
Epoch 59/100
 - 70s - loss: 0.0202 - acc: 0.9946 - val_loss: 0.7806 - val_acc: 0.8330
Epoch 60/100
 - 70s - loss: 0.0200 - acc: 0.9949 - val_loss: 0.9983 - val_acc: 0.8084
Epoch 61/100
 - 70s - loss: 0.0169 - acc: 0.9957 - val_loss: 0.8155 - val_acc: 0.8306
Epoch 62/100
 - 70s - loss: 0.0171 - acc: 0.9955 - val_loss: 0.8482 - val_acc: 0.8252
Epoch 63/100
 - 70s - loss: 0.0182 - acc: 0.9950 - val_loss: 0.8508 - val_acc: 0.8244
Epoch 64/100
 - 70s - loss: 0.0154 - acc: 0.9962 - val_loss: 0.8528 - val_acc: 0.8270
Epoch 65/100
 - 70s - loss: 0.0177 - acc: 0.9950 - val_loss: 0.8564 - val_acc: 0.8196
Epoch 66/100
 - 70s - loss: 0.0153 - acc: 0.9964 - val_loss: 0.8282 - val_acc: 0.8304
Epoch 67/100
 - 70s - loss: 0.0149 - acc: 0.9964 - val_loss: 0.8498 - val_acc: 0.8320
Epoch 68/100
 - 70s - loss: 0.0142 - acc: 0.9965 - val_loss: 0.8199 - val_acc: 0.8328
Epoch 69/100
 - 70s - loss: 0.0124 - acc: 0.9971 - val_loss: 0.8212 - val_acc: 0.8278
Epoch 70/100
 - 70s - loss: 0.0124 - acc: 0.9972 - val_loss: 0.8101 - val_acc: 0.8302
Epoch 71/100
 - 70s - loss: 0.0141 - acc: 0.9965 - val_loss: 0.7879 - val_acc: 0.8336
Epoch 72/100
 - 70s - loss: 0.0125 - acc: 0.9973 - val_loss: 0.8148 - val_acc: 0.8336
Epoch 73/100
 - 70s - loss: 0.0138 - acc: 0.9965 - val_loss: 0.9668 - val_acc: 0.8186
Epoch 74/100
 - 70s - loss: 0.0119 - acc: 0.9973 - val_loss: 0.7906 - val_acc: 0.8346
Epoch 75/100
 - 70s - loss: 0.0107 - acc: 0.9977 - val_loss: 0.8194 - val_acc: 0.8312
Epoch 76/100
 - 70s - loss: 0.0107 - acc: 0.9976 - val_loss: 0.8232 - val_acc: 0.8306
Epoch 77/100
 - 70s - loss: 0.0117 - acc: 0.9968 - val_loss: 0.8021 - val_acc: 0.8388
Epoch 78/100
 - 70s - loss: 0.0108 - acc: 0.9974 - val_loss: 0.8066 - val_acc: 0.8368
Epoch 79/100
 - 70s - loss: 0.0104 - acc: 0.9977 - val_loss: 0.8282 - val_acc: 0.8362
Epoch 80/100
 - 70s - loss: 0.0105 - acc: 0.9978 - val_loss: 0.8083 - val_acc: 0.8336
Epoch 81/100
 - 70s - loss: 0.0098 - acc: 0.9979 - val_loss: 0.8350 - val_acc: 0.8314
Epoch 82/100
 - 70s - loss: 0.0099 - acc: 0.9978 - val_loss: 0.9201 - val_acc: 0.8264
Epoch 83/100
 - 70s - loss: 0.0107 - acc: 0.9974 - val_loss: 0.8328 - val_acc: 0.8350
Epoch 84/100
 - 70s - loss: 0.0087 - acc: 0.9984 - val_loss: 0.8280 - val_acc: 0.8404
Epoch 85/100
 - 70s - loss: 0.0089 - acc: 0.9978 - val_loss: 0.8425 - val_acc: 0.8342
Epoch 86/100
 - 70s - loss: 0.0092 - acc: 0.9980 - val_loss: 0.8337 - val_acc: 0.8370
Epoch 87/100
 - 70s - loss: 0.0086 - acc: 0.9978 - val_loss: 0.8041 - val_acc: 0.8356
Epoch 88/100
 - 70s - loss: 0.0092 - acc: 0.9976 - val_loss: 0.8392 - val_acc: 0.8322
Epoch 89/100
 - 70s - loss: 0.0085 - acc: 0.9982 - val_loss: 0.8635 - val_acc: 0.8306
Epoch 90/100
 - 70s - loss: 0.0091 - acc: 0.9978 - val_loss: 0.7808 - val_acc: 0.8362
Epoch 91/100
 - 70s - loss: 0.0081 - acc: 0.9982 - val_loss: 0.8247 - val_acc: 0.8376
Epoch 92/100
 - 70s - loss: 0.0080 - acc: 0.9982 - val_loss: 0.8384 - val_acc: 0.8354
Epoch 93/100
 - 70s - loss: 0.0083 - acc: 0.9980 - val_loss: 0.8228 - val_acc: 0.8364
Epoch 94/100
 - 70s - loss: 0.0079 - acc: 0.9982 - val_loss: 0.8701 - val_acc: 0.8316
Epoch 95/100
 - 70s - loss: 0.0075 - acc: 0.9984 - val_loss: 0.8127 - val_acc: 0.8426
Epoch 96/100
 - 70s - loss: 0.0074 - acc: 0.9985 - val_loss: 0.8606 - val_acc: 0.8340
Epoch 97/100
 - 70s - loss: 0.0077 - acc: 0.9982 - val_loss: 0.9194 - val_acc: 0.8286
Epoch 98/100
 - 70s - loss: 0.0073 - acc: 0.9986 - val_loss: 0.8651 - val_acc: 0.8318
Epoch 99/100
 - 70s - loss: 0.0076 - acc: 0.9981 - val_loss: 0.8415 - val_acc: 0.8350
Epoch 100/100
 - 70s - loss: 0.0078 - acc: 0.9983 - val_loss: 0.8540 - val_acc: 0.8380
6997.34271812439
--------
{'val_loss': [1.6200977401733399, 1.5719944534301757, 1.8952482620239257, 1.246493750190735, 1.6433688011169434, 1.0184076147079468, 0.930525026512146, 0.8143343845367431, 1.2651877868652344, 1.0046515407562255, 0.8418880081176758, 1.0887598684310913, 0.8431494800567627, 1.0365229530334472, 0.9591659055709839, 0.6791117109298707, 0.9202235641479493, 0.7831443241119385, 0.8667891437530517, 0.7094350812911987, 0.7331427292823791, 0.7320328548431396, 0.946179364490509, 1.0807138427734375, 0.712634147644043, 0.7675963119506836, 0.9115697025299072, 0.7514156387329102, 0.7480964509963989, 0.8189453964233399, 1.6600189514160155, 0.9681376029968262, 0.8538184057235718, 0.7598634496688843, 0.7435412437438965, 0.8319998405456543, 0.9751784805297852, 0.7276888957977294, 0.80086969871521, 0.7915860750198365, 0.87705311794281, 0.7659943746566773, 1.020323601913452, 1.1547420146942138, 1.0170596374511718, 0.7629607173919678, 0.8480230882644654, 0.9309493312835694, 0.7897438144683838, 0.8316876983642578, 0.8173132188796997, 0.8424829887390137, 0.8760832090377808, 0.7941656017303467, 0.883848764038086, 0.7618085451126099, 1.0224832099914551, 0.7577600906372071, 0.7806433883666992, 0.9982980443954468, 0.8154537261962891, 0.8482130838394165, 0.8508461547851562, 0.8528203998565674, 0.856388980102539, 0.8281852195739746, 0.8498362823486328, 0.8199263397216797, 0.8211882230758667, 0.8101002620697022, 0.7879050937652587, 0.8147726528167725, 0.9668195993423462, 0.7906117164611817, 0.8194265216827392, 0.8231919883728027, 0.8021345918655396, 0.8066252439498901, 0.8281818710327148, 0.8083298046112061, 0.8349887111663818, 0.9201012981414795, 0.8327812261581421, 0.8279685579299927, 0.8425069679260254, 0.8337120073318481, 0.8040606538772583, 0.8391816539764404, 0.8634979557037353, 0.7807951265335084, 0.8246621097564697, 0.8383606002807618, 0.8228445766448974, 0.8700928081512451, 0.8127269742965698, 0.8605683334350586, 0.9193765480041504, 0.8650864286422729, 0.84148745803833, 0.8540468025207519], 'val_acc': [0.415, 0.434, 0.4034, 0.583, 0.4778, 0.6518, 0.6768, 0.717, 0.586, 0.6564, 0.7238, 0.6566, 0.724, 0.6578, 0.7076, 0.7744, 0.727, 0.7446, 0.7408, 0.7836, 0.7772, 0.774, 0.7422, 0.7302, 0.783, 0.7812, 0.7544, 0.794, 0.7954, 0.778, 0.6734, 0.7706, 0.7914, 0.8118, 0.81, 0.8068, 0.7832, 0.8158, 0.8112, 0.8158, 0.8072, 0.8266, 0.7776, 0.7696, 0.7788, 0.8232, 0.8152, 0.795, 0.826, 0.819, 0.825, 0.8216, 0.8148, 0.8272, 0.8138, 0.8338, 0.7988, 0.8372, 0.833, 0.8084, 0.8306, 0.8252, 0.8244, 0.827, 0.8196, 0.8304, 0.832, 0.8328, 0.8278, 0.8302, 0.8336, 0.8336, 0.8186, 0.8346, 0.8312, 0.8306, 0.8388, 0.8368, 0.8362, 0.8336, 0.8314, 0.8264, 0.835, 0.8404, 0.8342, 0.837, 0.8356, 0.8322, 0.8306, 0.8362, 0.8376, 0.8354, 0.8364, 0.8316, 0.8426, 0.834, 0.8286, 0.8318, 0.835, 0.838], 'loss': [3.531012087419298, 1.545546928554111, 1.3235138313505386, 1.141941246647305, 0.9910100009812249, 0.88976975440979, 0.8149735055287679, 0.7532310496966044, 0.6917872505293952, 0.6461721091164483, 0.5959709793408712, 0.5558003814909194, 0.520109560447269, 0.48326839338938393, 0.45149458472993637, 0.4180825393888685, 0.3908382562637329, 0.3626345124774509, 0.33373424081802366, 0.3109592623498705, 0.28686285332043965, 0.26394111314879526, 0.2404550051159329, 0.2209401857852936, 0.20237853852907817, 0.18630385750134787, 0.16644126212861804, 0.14931010817951626, 0.1391851197242737, 0.128664521212048, 0.11408744339942932, 0.10345182478162977, 0.09807709164089627, 0.08781021649042765, 0.07954031504789988, 0.0763856762568156, 0.06661913453406758, 0.06137754388815827, 0.05867686626282003, 0.05148977024422752, 0.04896049821972847, 0.04740802020596133, 0.04289711684650845, 0.04071995440059238, 0.036859797337320116, 0.03534514444669088, 0.035370508591334025, 0.03341683094965087, 0.02994220248626338, 0.02835041825208399, 0.027569894361827108, 0.02564560255739424, 0.023861959672636456, 0.024975499362415738, 0.021792908370494843, 0.02252843091554112, 0.021144410247272916, 0.020899681885374916, 0.020177618713180225, 0.019951286415259045, 0.016926520741648143, 0.01705110257797771, 0.018170323442088233, 0.01536156833337413, 0.017715420043468474, 0.015270502183503575, 0.014909092631729112, 0.014167917176584402, 0.012390891187886397, 0.01236555186605288, 0.014106888800859451, 0.012470278079393837, 0.013817076112164392, 0.01194907361658083, 0.010663731824689441, 0.010740930540694131, 0.011721519207954407, 0.010815695669253667, 0.010391595080329313, 0.010544545126913323, 0.009775434821844101, 0.009942335274732775, 0.01065157656951083, 0.008712903178110718, 0.008907177431881427, 0.00918683369933731, 0.008612545613613394, 0.00919340512437953, 0.008486308809287019, 0.009076146108574337, 0.008053226922411057, 0.007976949091917939, 0.00828039533806344, 0.00788304032302565, 0.007492423144645161, 0.007424667356659968, 0.0077271151130398116, 0.007264928127162986, 0.007643222833673159, 0.00782724274057481], 'acc': [0.307933333322737, 0.47493333337571886, 0.546133333322737, 0.5971333333651225, 0.6460444444762335, 0.6846222222646078, 0.7112222221904331, 0.7344888889312744, 0.7549111111534966, 0.7717555555237664, 0.7897777777883742, 0.8051777777565851, 0.8186666666560702, 0.8299111111534966, 0.8418000000105964, 0.8544222222116259, 0.863866666645474, 0.8738888888994852, 0.8825111111429003, 0.8920666666348775, 0.9002222222010294, 0.9094444444762336, 0.9165111111217075, 0.9230000000211928, 0.9316222221798367, 0.937311111079322, 0.9437111110687256, 0.9498666666348775, 0.9540666667090522, 0.9570888889312744, 0.962577777756585, 0.9657999999682109, 0.968044444402059, 0.971111111079322, 0.974866666645474, 0.9752222222116258, 0.9783555555449591, 0.9813111111111111, 0.9827111111111111, 0.9851999999788073, 0.985777777756585, 0.9854444444444445, 0.987377777756585, 0.988377777756585, 0.9894444444126553, 0.9898888888676961, 0.9890888888888889, 0.9903999999788072, 0.9919111111111111, 0.9920666666560702, 0.9923333333333333, 0.9931333333333333, 0.9939333333121406, 0.9930888888888889, 0.9945111111005147, 0.9938, 0.9947777777671813, 0.9945555555555555, 0.9946, 0.9948888888782925, 0.9956666666560703, 0.9955333333333334, 0.9950222222010294, 0.9962, 0.9950222222010294, 0.9963555555555555, 0.9963777777777778, 0.9964888888888889, 0.9971333333333333, 0.9971777777777778, 0.9964666666666666, 0.9972888888888889, 0.9965111111111111, 0.9972888888888889, 0.9977111111111111, 0.9975777777671814, 0.9968, 0.9973777777777778, 0.9976888888888888, 0.9977777777777778, 0.9979333333333333, 0.9978, 0.9973555555555556, 0.9984, 0.997844444433848, 0.9979777777777777, 0.9978444444444444, 0.9975555555555555, 0.9982, 0.9977777777777778, 0.9982444444444445, 0.9981777777777778, 0.9980444444444444, 0.9982444444444445, 0.9984222222222222, 0.9985111111111111, 0.9981555555449592, 0.9985555555555555, 0.9981333333227369, 0.9982666666666666]}
===Final Test Score===
Test loss: 0.8246918324708938
Test accuracy: 0.8412
